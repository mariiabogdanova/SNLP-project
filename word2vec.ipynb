{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3eaaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariiabogdanova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mariiabogdanova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading English model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967204e3a6f5485cadd0c09d3851186b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 17:44:49 INFO: Downloading default packages for language: en (English)...\n",
      "2022-04-27 17:44:50 INFO: File exists: /Users/mariiabogdanova/stanza_resources/en/default.zip.\n",
      "2022-04-27 17:44:54 INFO: Finished downloading models and saved to /Users/mariiabogdanova/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd03482",
   "metadata": {},
   "source": [
    "# Document Similarity: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a3d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(doc_str, stopwords):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words.\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    doc = nlp(doc_str)\n",
    "    words = doc.iter_words()\n",
    "    normalized_doc = []\n",
    "    for w in words:\n",
    "        w = w.lemma.lower()\n",
    "        if not w in stopwords:\n",
    "            normalized_doc.append(w)\n",
    "    normalized_doc = ' '.join(normalized_doc)\n",
    "    return normalized_doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30bffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the numbers at the start and end of the documents.\n",
    "DATAFILE = \"./Data/LeePincombeWelshDocuments.txt\"\n",
    "CLEANFILE = \"./Data/cleanLPW.txt\"\n",
    "INDIVIDUAL_DOCS = \"./Data/Docs\"\n",
    "stopwords_english = stopwords.words('english')\n",
    "normalized_docs = []\n",
    "if (os.path.exists(CLEANFILE)):\n",
    "    os.remove(CLEANFILE)\n",
    "i = 0\n",
    "with open(DATAFILE, 'r', encoding=\"utf8\", errors=\"ignore\") as inputfile:\n",
    "     lines = inputfile.readlines()\n",
    "     for line in lines[1:-1]:\n",
    "        start_removed = re.sub(\"(\\d*\\.\\s)\", \"\", line, 1)\n",
    "        end_removed = re.sub(\"\\(\\d* words\\)\", \"\", start_removed, 1)\n",
    "        normalized_docs.append(tokenize_and_normalize(end_removed, stopwords_english).split())\n",
    "        with open(CLEANFILE, 'a+') as outputfile:\n",
    "            outputfile.write(end_removed)\n",
    "        with open(INDIVIDUAL_DOCS+f\"/{i}.txt\", \"w+\") as docfile:\n",
    "            docfile.write(end_removed)\n",
    "            i = i + 1\n",
    "\n",
    "documents = []\n",
    "for doc in normalized_docs:\n",
    "    documents.append(' '.join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c2e428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_1</th>\n",
       "      <th>Document_2</th>\n",
       "      <th>Similarity_avg</th>\n",
       "      <th>Similarity_avg_normalized</th>\n",
       "      <th>Similarity_word2vec</th>\n",
       "      <th>Similarity_doc2vec</th>\n",
       "      <th>Similarity_tf_idf</th>\n",
       "      <th>Similarities_SBERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.412920</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.226575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.512574</td>\n",
       "      <td>0.224046</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.249689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.279664</td>\n",
       "      <td>0.201932</td>\n",
       "      <td>0.028945</td>\n",
       "      <td>0.231258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.372543</td>\n",
       "      <td>0.225909</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.160115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.250421</td>\n",
       "      <td>0.375090</td>\n",
       "      <td>0.013378</td>\n",
       "      <td>0.189578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_1  Document_2  Similarity_avg  Similarity_avg_normalized  \\\n",
       "0           1           2             1.5                      0.125   \n",
       "1           1           3             1.2                      0.050   \n",
       "2           1           4             1.0                      0.000   \n",
       "3           1           5             1.5                      0.125   \n",
       "4           1           6             2.5                      0.375   \n",
       "\n",
       "   Similarity_word2vec  Similarity_doc2vec  Similarity_tf_idf  \\\n",
       "0             0.202667            0.412920           0.021084   \n",
       "1             0.512574            0.224046           0.004666   \n",
       "2             0.279664            0.201932           0.028945   \n",
       "3             0.372543            0.225909           0.001599   \n",
       "4             0.250421            0.375090           0.013378   \n",
       "\n",
       "   Similarities_SBERT  \n",
       "0            0.226575  \n",
       "1            0.249689  \n",
       "2            0.231258  \n",
       "3            0.160115  \n",
       "4            0.189578  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_evaluation_data = pd.read_csv(\"Data/AverageSimilarities_fixed.csv\")\n",
    "human_evaluation_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449c5007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national executive strife-torn democrat last n...</td>\n",
       "      <td>national executive strife torn democrat last n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cash-strapped financial service group amp shel...</td>\n",
       "      <td>cash strapped financial service group amp shel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>united state government say want see president...</td>\n",
       "      <td>united state government say want see president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>radical armed islamist group tie tehran baghda...</td>\n",
       "      <td>radical armed islamist group tie tehran baghda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>washington sharply rebuked russia bombing geor...</td>\n",
       "      <td>washington sharply rebuked russia bombing geor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  \\\n",
       "0  national executive strife-torn democrat last n...   \n",
       "1  cash-strapped financial service group amp shel...   \n",
       "2  united state government say want see president...   \n",
       "3  radical armed islamist group tie tehran baghda...   \n",
       "4  washington sharply rebuked russia bombing geor...   \n",
       "\n",
       "                                   documents_cleaned  \n",
       "0  national executive strife torn democrat last n...  \n",
       "1  cash strapped financial service group amp shel...  \n",
       "2  united state government say want see president...  \n",
       "3  radical armed islamist group tie tehran baghda...  \n",
       "4  washington sharply rebuked russia bombing geor...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df = pd.DataFrame(documents,columns=['documents'])\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "documents_df['documents_cleaned'] = documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stopwords_list) )\n",
    "documents_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9f340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(w2v_model, doc: str) -> np.ndarray:\n",
    "    doc = doc.lower()\n",
    "    words = [w for w in doc.split(\" \")]\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = w2v_model[word]\n",
    "            word_vecs.append(vec)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    vector = np.mean(word_vecs, axis=0)\n",
    "    return vector\n",
    "\n",
    "def _cosine_sim(vecA, vecB):\n",
    "    csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "    if np.isnan(np.sum(csim)):\n",
    "        return 0\n",
    "    return csim\n",
    "\n",
    "def calculate_similarity(w2v_model, source_doc, target_docs=None, threshold=0):\n",
    "    if not target_docs:\n",
    "        return []\n",
    "\n",
    "    if isinstance(target_docs, str):\n",
    "        target_docs = [target_docs]\n",
    "\n",
    "    source_vec = vectorize(w2v_model, source_doc)\n",
    "    results = []\n",
    "    for doc in target_docs:\n",
    "        target_vec = vectorize(w2v_model, doc)\n",
    "        sim_score = _cosine_sim(source_vec, target_vec)\n",
    "        if sim_score > threshold:\n",
    "            results.append({\"score\": sim_score, \"doc\": doc})\n",
    "        results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0395af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download pre-trained word2vec here https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe27ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for doc1, doc2 in zip(human_evaluation_data.Document_1.values, human_evaluation_data.Document_2.values):\n",
    "    sim_scores = calculate_similarity(w2v_model, documents_df.documents_cleaned.values[doc1-1], documents_df.documents_cleaned.values[doc2-1])\n",
    "    data.append(sim_scores[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa53d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_evaluation_data[\"Similarity_word2vec\"] = data\n",
    "human_evaluation_data.to_csv('Data/AverageSimilarities_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cff930c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6116588694445468"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr(human_evaluation_data.Similarity_avg, human_evaluation_data.Similarity_word2vec)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
