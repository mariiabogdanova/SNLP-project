{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954117be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariiabogdanova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f061fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range (0, 50):\n",
    "    FILENAME = \"Data/Docs/{}.txt\".format(i)\n",
    "    with open(FILENAME, 'r', encoding=\"utf8\", errors=\"ignore\") as inputfile:\n",
    "        lines = inputfile.readlines()\n",
    "        for line in lines:\n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780114c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Similarity: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449c5007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The national executive of the strife-torn Demo...</td>\n",
       "      <td>national executive strife torn democrats last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cash-strapped financial services group AMP has...</td>\n",
       "      <td>cash strapped financial services group amp she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The United States government has said it wants...</td>\n",
       "      <td>united states government said wants see presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A radical armed Islamist group with ties to Te...</td>\n",
       "      <td>radical armed islamist group ties tehran baghd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington has sharply rebuked Russia over bom...</td>\n",
       "      <td>washington sharply rebuked russia bombings geo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  \\\n",
       "0  The national executive of the strife-torn Demo...   \n",
       "1  Cash-strapped financial services group AMP has...   \n",
       "2  The United States government has said it wants...   \n",
       "3  A radical armed Islamist group with ties to Te...   \n",
       "4  Washington has sharply rebuked Russia over bom...   \n",
       "\n",
       "                                   documents_cleaned  \n",
       "0  national executive strife torn democrats last ...  \n",
       "1  cash strapped financial services group amp she...  \n",
       "2  united states government said wants see presid...  \n",
       "3  radical armed islamist group ties tehran baghd...  \n",
       "4  washington sharply rebuked russia bombings geo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df = pd.DataFrame(documents,columns=['documents'])\n",
    "\n",
    "# removing special characters and stop words from the text\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "documents_df['documents_cleaned'] = documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stopwords_list) )\n",
    "documents_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648c90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectoriser = TfidfVectorizer(max_features = 64)\n",
    "tfidfvectoriser.fit(documents_df.documents_cleaned)\n",
    "tfidf_vectors = tfidfvectoriser.transform(documents_df.documents_cleaned)\n",
    "tfidf_vectors = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c24329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203  65 455 456  39   3  21 204 205 457 103  28  15 206 104 105  22 458\n",
      " 106 459 460 461   7   1 107 462 106 463 207   7   1  66 107 203  65   3\n",
      "  21 208 209 108   1 210 211 105  22 212  15 104 464 465  22 109 110 111\n",
      " 466 213 214 467   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents = tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents = pad_sequences(tokenized_documents,maxlen = 64,padding = 'post')\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "print(tokenized_paded_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff6dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020c115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariiabogdanova/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if word in model_w2v:\n",
    "        embedding_matrix[i] = model_w2v[word]\n",
    "        \n",
    "# creating document-word embeddings\n",
    "document_word_embeddings = np.zeros((len(tokenized_paded_documents),64,300))\n",
    "\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(tokenized_paded_documents[0])):\n",
    "        document_word_embeddings[i][j] = embedding_matrix[tokenized_paded_documents[i][j]]\n",
    "        \n",
    "# tf-idf vectors do not keep the original sequence of words, converting them into actual word sequences from the documents\n",
    "document_embeddings = np.zeros((len(tokenized_paded_documents),300))\n",
    "words = tfidfvectoriser.get_feature_names()\n",
    "\n",
    "for i in range(len(document_word_embeddings)):\n",
    "    for j in range(len(words)):\n",
    "        document_embeddings[i] += embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "        \n",
    "document_embeddings_word2vec = document_embeddings/np.sum(tfidf_vectors,axis=1).reshape(-1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "904aca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities_word2vec = cosine_similarity(document_embeddings_word2vec)\n",
    "\n",
    "# with open('Binaries/pairwise_similarities_word2vec.pickle','wb') as f:\n",
    "#     pickle.dump(pairwise_similarities_word2vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5df42d",
   "metadata": {},
   "source": [
    "# Document Similarity: doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a97147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-1ce516575887>:10: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  document_embeddings_doc2vec[i] = model_d2v.docvecs[i]\n"
     ]
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words = word_tokenize(doc), tags = [i]) for i, doc in enumerate(documents_df.documents_cleaned)]\n",
    "\n",
    "model_d2v = Doc2Vec(vector_size = 100, alpha = 0.025, min_count=1)\n",
    "model_d2v.build_vocab(tagged_data)\n",
    "\n",
    "model_d2v.train(tagged_data, total_examples = model_d2v.corpus_count, epochs = 200)\n",
    "\n",
    "document_embeddings_doc2vec = np.zeros((documents_df.shape[0],100))\n",
    "for i in range(len(document_embeddings)):\n",
    "    document_embeddings_doc2vec[i] = model_d2v.docvecs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9629e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities_doc2vec = cosine_similarity(document_embeddings_doc2vec)\n",
    "\n",
    "# with open('Binaries/pairwise_similarities_doc2vec.pickle','wb') as f:\n",
    "#     pickle.dump(pairwise_similarities_doc2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72be23fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_1</th>\n",
       "      <th>Document_2</th>\n",
       "      <th>Similarity_avg</th>\n",
       "      <th>Similarity_avg_normalized</th>\n",
       "      <th>Similarity_word2vec</th>\n",
       "      <th>Similarity_doc2vec</th>\n",
       "      <th>Similarity_tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.412920</td>\n",
       "      <td>0.021084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.512574</td>\n",
       "      <td>0.224046</td>\n",
       "      <td>0.004666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.279664</td>\n",
       "      <td>0.201932</td>\n",
       "      <td>0.028945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.372543</td>\n",
       "      <td>0.225909</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.250421</td>\n",
       "      <td>0.375090</td>\n",
       "      <td>0.013378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.266212</td>\n",
       "      <td>0.253162</td>\n",
       "      <td>0.017224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.291865</td>\n",
       "      <td>0.346643</td>\n",
       "      <td>0.020166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461878</td>\n",
       "      <td>0.529536</td>\n",
       "      <td>0.065105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.308305</td>\n",
       "      <td>0.205616</td>\n",
       "      <td>0.003985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.350480</td>\n",
       "      <td>0.157230</td>\n",
       "      <td>0.003457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_1  Document_2  Similarity_avg  Similarity_avg_normalized  \\\n",
       "0           1           2             1.5                      0.125   \n",
       "1           1           3             1.2                      0.050   \n",
       "2           1           4             1.0                      0.000   \n",
       "3           1           5             1.5                      0.125   \n",
       "4           1           6             2.5                      0.375   \n",
       "5           1           7             1.3                      0.075   \n",
       "6           1           8             1.2                      0.050   \n",
       "7           1           9             1.0                      0.000   \n",
       "8           1          10             1.3                      0.075   \n",
       "9           1          11             1.3                      0.075   \n",
       "\n",
       "   Similarity_word2vec  Similarity_doc2vec  Similarity_tf_idf  \n",
       "0             0.202667            0.412920           0.021084  \n",
       "1             0.512574            0.224046           0.004666  \n",
       "2             0.279664            0.201932           0.028945  \n",
       "3             0.372543            0.225909           0.001599  \n",
       "4             0.250421            0.375090           0.013378  \n",
       "5             0.266212            0.253162           0.017224  \n",
       "6             0.291865            0.346643           0.020166  \n",
       "7             0.461878            0.529536           0.065105  \n",
       "8             0.308305            0.205616           0.003985  \n",
       "9             0.350480            0.157230           0.003457  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving scores to csv\n",
    "human_evaluation_data = pd.read_csv(\"Data/AverageSimilarities_fixed.csv\")\n",
    "human_evaluation_data[\"Similarity_word2vec\"] = pairwise_similarities_word2vec[human_evaluation_data.Document_1-1, human_evaluation_data.Document_2-1]\n",
    "human_evaluation_data[\"Similarity_doc2vec\"] = pairwise_similarities_doc2vec[human_evaluation_data.Document_1-1, human_evaluation_data.Document_2-1]\n",
    "human_evaluation_data.head(5)\n",
    "\n",
    "human_evaluation_data.to_csv('Data/AverageSimilarities_fixed.csv', index=False)\n",
    "human_evaluation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4796568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.41866115],\n",
       "       [0.41866115, 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(human_evaluation_data.Similarity_avg, human_evaluation_data.Similarity_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e15c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
