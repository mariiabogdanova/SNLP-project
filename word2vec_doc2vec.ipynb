{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "954117be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariiabogdanova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43c99a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANFILE = \"./Data/cleanLPW.txt\"\n",
    "\n",
    "documents = []\n",
    "with open(CLEANFILE, 'r', encoding=\"utf8\", errors=\"ignore\") as inputfile:\n",
    "    lines = inputfile.readlines()\n",
    "    for line in lines[1:-1]:\n",
    "        documents.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780114c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Similarity: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "449c5007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The national executive of the strife-torn Demo...</td>\n",
       "      <td>national executive strife torn democrats last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cash-strapped financial services group AMP has...</td>\n",
       "      <td>cash strapped financial services group amp she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The United States government has said it wants...</td>\n",
       "      <td>united states government said wants see presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A radical armed Islamist group with ties to Te...</td>\n",
       "      <td>radical armed islamist group ties tehran baghd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington has sharply rebuked Russia over bom...</td>\n",
       "      <td>washington sharply rebuked russia bombings geo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  \\\n",
       "0  The national executive of the strife-torn Demo...   \n",
       "1  Cash-strapped financial services group AMP has...   \n",
       "2  The United States government has said it wants...   \n",
       "3  A radical armed Islamist group with ties to Te...   \n",
       "4  Washington has sharply rebuked Russia over bom...   \n",
       "\n",
       "                                   documents_cleaned  \n",
       "0  national executive strife torn democrats last ...  \n",
       "1  cash strapped financial services group amp she...  \n",
       "2  united states government said wants see presid...  \n",
       "3  radical armed islamist group ties tehran baghd...  \n",
       "4  washington sharply rebuked russia bombings geo...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df = pd.DataFrame(documents,columns=['documents'])\n",
    "\n",
    "# removing special characters and stop words from the text\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "documents_df['documents_cleaned'] = documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stopwords_list) )\n",
    "documents_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "648c90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectoriser = TfidfVectorizer(max_features = 64)\n",
    "tfidfvectoriser.fit(documents_df.documents_cleaned)\n",
    "tfidf_vectors = tfidfvectoriser.transform(documents_df.documents_cleaned)\n",
    "tfidf_vectors = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7c24329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190  60 424 425  37   4  19 191 192 426 193  27  14 194  93  94  20 427\n",
      "  95 428 429 430  10   1  96 431  95 432 195  10   1  61  96 190  60   4\n",
      "  19 196 197  97   1 198 199  94  20 200  14  93 433 434  20  98  99 100\n",
      " 435 201 202 436   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad every document to make them of the same size\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "tokenized_documents = tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "tokenized_paded_documents = pad_sequences(tokenized_documents,maxlen = 64,padding = 'post')\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "print(tokenized_paded_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ff6dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"Binaries/GoogleNews-vectors-negative300.bin.gz\"\n",
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "020c115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariiabogdanova/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if word in model_w2v:\n",
    "        embedding_matrix[i] = model_w2v[word]\n",
    "        \n",
    "# creating document-word embeddings\n",
    "document_word_embeddings = np.zeros((len(tokenized_paded_documents),64,300))\n",
    "\n",
    "for i in range(len(tokenized_paded_documents)):\n",
    "    for j in range(len(tokenized_paded_documents[0])):\n",
    "        document_word_embeddings[i][j] = embedding_matrix[tokenized_paded_documents[i][j]]\n",
    "        \n",
    "# tf-idf vectors do not keep the original sequence of words, converting them into actual word sequences from the documents\n",
    "document_embeddings = np.zeros((len(tokenized_paded_documents),300))\n",
    "words = tfidfvectoriser.get_feature_names()\n",
    "\n",
    "for i in range(len(document_word_embeddings)):\n",
    "    for j in range(len(words)):\n",
    "        document_embeddings[i] += embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "        \n",
    "document_embeddings_word2vec = document_embeddings/np.sum(tfidf_vectors,axis=1).reshape(-1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "904aca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities_word2vec = cosine_similarity(document_embeddings_word2vec)\n",
    "\n",
    "# with open('Binaries/pairwise_similarities_word2vec.pickle','wb') as f:\n",
    "#     pickle.dump(pairwise_similarities_word2vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5df42d",
   "metadata": {},
   "source": [
    "# Document Similarity: doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47a97147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-2999ae2f9c8b>:11: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  document_embeddings_doc2vec[i] = model_d2v.docvecs[i]\n"
     ]
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words = word_tokenize(doc), tags = [i]) for i, doc in enumerate(documents_df.documents_cleaned)]\n",
    "\n",
    "model_d2v = Doc2Vec(vector_size = 100, alpha = 0.025, min_count=1)\n",
    "model_d2v.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model_d2v.train(tagged_data, total_examples = model_d2v.corpus_count, epochs = model_d2v.epochs)\n",
    "\n",
    "document_embeddings_doc2vec = np.zeros((documents_df.shape[0],100))\n",
    "for i in range(len(document_embeddings)):\n",
    "    document_embeddings_doc2vec[i] = model_d2v.docvecs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9629e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarities_doc2vec = cosine_similarity(document_embeddings_doc2vec)\n",
    "\n",
    "# with open('Binaries/pairwise_similarities_doc2vec.pickle','wb') as f:\n",
    "#     pickle.dump(pairwise_similarities_doc2vec, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
